name: CI • GPU (CUDA)

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - ".github/workflows/**"
      - "app/**"
      - "tests/**"
  pull_request:
    paths:
      - ".github/workflows/**"
      - "app/**"
      - "tests/**"

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  PYTHONUNBUFFERED: "1"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  APP_RELOAD: "false"
  APP_DEVICE: "cuda:0"
  HF_HOME: ${{ github.workspace }}/models_cache/huggingface
  TORCH_HOME: ${{ github.workspace }}/models_cache/torch
  TRANSFORMERS_CACHE: ${{ github.workspace }}/models_cache/huggingface/hub

jobs:
  windows-gpu:
    name: Windows GPU • py${{ matrix.python }}
    runs-on: [self-hosted, windows, gpu, cuda]
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        python: ['3.12']

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}
          cache: pip

      - name: Cache model caches (HF/Torch)
        uses: actions/cache@v4
        with:
          path: |
            models_cache/huggingface
            models_cache/torch
          key: windows-gpu-caches-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            windows-gpu-caches-

      - name: Install deps
        run: |
          python -m pip install -U pip ruff
          pip install -r requirements.txt
          if exist requirements-dev.txt ( pip install -r requirements-dev.txt )

      - name: Install PyTorch CUDA wheels
        run: |
          pip install --extra-index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio

      - name: Verify CUDA is available
        run: |
          python - << 'PY'
          import torch, sys
          print("CUDA available:", torch.cuda.is_available())
          print("CUDA device count:", torch.cuda.device_count())
          sys.exit(0 if torch.cuda.is_available() else 1)
          PY

      - name: Ruff (lint + fmt check)
        run: |
          ruff --version
          ruff check .
          ruff format --check

      - name: Pytest (GPU tests)
        env:
          DEVICE: cuda
        run: |
          pytest -q -m "gpu or gpu_cuda" --maxfail=1 -k "not slow" --durations=10

      - name: Smoke test API on GPU (/health)
        env:
          DEVICE: cuda
        run: |
          python scripts/smoke_test.py

      - name: Upload logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: logs-windows-gpu
          path: |
            logs/**/*.log
            .pytest_cache/**
            .ruff_cache/**
