name: CI • GPU (CUDA)

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - ".github/workflows/**"
      - "app/**"
      - "tests/**"
  pull_request:
    paths:
      - ".github/workflows/**"
      - "app/**"
      - "tests/**"

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

env:
  PYTHONUNBUFFERED: "1"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  APP_RELOAD: "false"
  APP_DEVICE: "cuda:0"

  HF_HOME: ${{ github.workspace }}/models_cache/huggingface
  TORCH_HOME: ${{ github.workspace }}/models_cache/torch
  TRANSFORMERS_CACHE: ${{ github.workspace }}/models_cache/huggingface/hub

jobs:

  windows-gpu:
    name: Windows GPU • py${{ matrix.python }}

    runs-on: [self-hosted, windows, gpu, cuda]
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        python: ['3.12']

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}
          cache: pip

      - name: Cache model caches (HF/Torch)
        uses: actions/cache@v4
        with:
          path: |
            models_cache/huggingface
            models_cache/torch
          key: ${{ runner.os }}-gpu-caches-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-gpu-caches-

      - name: Install deps
        shell: powershell
        run: |
          python -m pip install -U pip ruff
          pip install -r requirements.txt
          if (Test-Path requirements-dev.txt) { pip install -r requirements-dev.txt }

      - name: Install PyTorch CUDA wheels
        shell: powershell
        run: |

          pip install --extra-index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio

      - name: Verify CUDA is available
        shell: powershell
        run: |
          python - << 'PY'
          import torch, sys
          print("CUDA available:", torch.cuda.is_available())
          print("CUDA device count:", torch.cuda.device_count())
          sys.exit(0 if torch.cuda.is_available() else 1)
          PY

      - name: Ruff (lint + fmt check)
        shell: powershell
        run: |
          ruff --version
          ruff check .
          ruff format --check

      - name: Pytest (GPU tests)
        shell: powershell
        env:
          DEVICE: cuda
        run: |

          pytest -q -m "gpu or gpu_cuda" --maxfail=1 -k "not slow" --durations=10

      - name: Smoke test API on GPU (/health)
        shell: powershell
        env:
          DEVICE: cuda
        run: |
          $proc = Start-Process -FilePath uvicorn -ArgumentList "app.main:app --host 127.0.0.1 --port 8001" -PassThru
          try {
            $ok = $false
            for ($i = 0; $i -lt 40; $i++) {
              try {
                $r = Invoke-WebRequest -Uri "http://127.0.0.1:8001/health" -TimeoutSec 2 -UseBasicParsing
                if ($r.StatusCode -eq 200) { $ok = $true; break }
              } catch { Start-Sleep -Milliseconds 500 }
            }
            if (-not $ok) { exit 1 }
          } finally {
            if ($proc) { Stop-Process -Id $proc.Id -Force }
          }

      - name: Upload logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: logs-windows-gpu
          path: |
            logs/**/*.log
            .pytest_cache/**
            .ruff_cache/**


  ubuntu-gpu:
    name: Ubuntu GPU • py${{ matrix.python }}
    runs-on: [self-hosted, linux, gpu, cuda]
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        python: ['3.12']
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}
          cache: pip

      - name: Cache model caches (HF/Torch)
        uses: actions/cache@v4
        with:
          path: |
            models_cache/huggingface
            models_cache/torch
          key: linux-gpu-caches-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            linux-gpu-caches-

      - name: Install deps
        shell: bash
        run: |
          python -m pip install -U pip ruff
          pip install -r requirements.txt
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi

      - name: Install PyTorch CUDA wheels
        shell: bash
        run: |
          pip install --extra-index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio

      - name: Verify CUDA is available
        shell: bash
        run: |
          python - << 'PY'
          import torch, sys
          print("CUDA available:", torch.cuda.is_available())
          print("CUDA device count:", torch.cuda.device_count())
          sys.exit(0 if torch.cuda.is_available() else 1)
          PY

      - name: Ruff (lint + fmt check)
        shell: bash
        run: |
          ruff --version
          ruff check .
          ruff format --check

      - name: Pytest (GPU tests)
        shell: bash
        env:
          DEVICE: cuda
        run: |
          pytest -q -m "gpu or gpu_cuda" --maxfail=1 -k "not slow" --durations=10

      - name: Smoke test API on GPU (/health)
        shell: bash
        env:
          DEVICE: cuda
        run: |
          uvicorn app.main:app --host 127.0.0.1 --port 8001 &
          pid=$!
          python - << 'PY'
          import time, urllib.request, sys
          for _ in range(40):
              try:
                  with urllib.request.urlopen('http://127.0.0.1:8001/health', timeout=2) as r:
                      print(r.read())
                      sys.exit(0)
              except Exception:
                  time.sleep(0.5)
          sys.exit(1)
          PY
          kill $pid || true
