# ğŸš€ NeuroServe â€” GPUâ€‘Ready FastAPI AI Server

<p align="left">
  <a href="https://www.python.org/"><img alt="Python" src="https://img.shields.io/badge/Python-3.12%2B-blue" /></a>
  <a href="https://fastapi.tiangolo.com/"><img alt="FastAPI" src="https://img.shields.io/badge/FastAPI-0.116.x-009688" /></a>
  <a href="https://pytorch.org/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-2.6.x-ee4c2c" /></a>

  <!-- CI badges -->
  <a href="https://github.com/USERNAME/gpu-server/actions/workflows/ci-ubuntu.yml">
    <img alt="Ubuntu CI" src="https://github.com/USERNAME/gpu-server/actions/workflows/ci-ubuntu.yml/badge.svg" />
  </a>
  <a href="https://github.com/USERNAME/gpu-server/actions/workflows/ci-macos.yml">
    <img alt="macOS CI" src="https://github.com/USERNAME/gpu-server/actions/workflows/ci-macos.yml/badge.svg" />
  </a>
  <a href="https://github.com/USERNAME/gpu-server/actions/workflows/ci-windows.yml">
    <img alt="Windows CI" src="https://github.com/USERNAME/gpu-server/actions/workflows/ci-windows.yml/badge.svg" />
  </a>

  <a href="./LICENSE"><img alt="License" src="https://img.shields.io/badge/License-MIT-green" /></a>
</p>


NeuroServe is an **AI Inference Server** built on **FastAPI** and designed to run seamlessly on **GPU / CPU / ROCm / macOS MPS**.  
It provides ready-to-use REST APIs, a plugin system, prefetch utilities for models, and runtime inspection tools.

---

## âœ¨ Key Features
- ğŸŒ **Ready REST APIs** with Swagger UI (`/docs`) & ReDoc (`/redoc`).
- âš¡ **PyTorch integration** with auto device selection (CUDA/CPU/MPS/ROCm).
- ğŸ§© **Plugin system** for loading and running models or services as standalone plugins.
- ğŸ“Š **Runtime utilities**: CUDA info and warmup routines.
- ğŸ§  **Model tools**: TinyNet example model & MLP memory size calculator.
- ğŸ§± **Unified responses** with `unify_response` for consistent API outputs.

---

## ğŸ“‚ Project Structure
```text
gpu-server/
â”œâ”€ app/
â”‚  â”œâ”€ main.py           # FastAPI entrypoint
â”‚  â”œâ”€ runtime.py        # Device (CUDA/CPU/MPS) management & info
â”‚  â”œâ”€ toy_model.py      # Simple PyTorch model
â”‚  â”œâ”€ core/             # config + logging + error handling
â”‚  â”œâ”€ routes/           # API routes (plugins, uploads, ...)
â”‚  â”œâ”€ plugins/          # plugin system (dummy, neu_server)
â”‚  â”œâ”€ templates/        # index.html
â”‚  â””â”€ static/           # style.css, favicon.ico
â”œâ”€ scripts/             # helper scripts (install_torch, prefetch_models, tests)
â”œâ”€ models_cache/        # HF/Torch model cache
â”œâ”€ docs/                # model licenses & docs
â”œâ”€ tests/               # optional tests
â”œâ”€ requirements*.txt
â”œâ”€ pyproject.toml       # Ruff/pytest/coverage configs
â””â”€ README.md , LICENSE
```

---

## âš™ï¸ Quick Installation
### 1) Clone the repository
```bash
git clone https://github.com/USERNAME/gpu-server.git
cd gpu-server
```

### 2) Create virtual environment
```bash
python -m venv .venv
# Linux/macOS
source .venv/bin/activate
# Windows
.venv\Scripts\activate
```

### 3) Install dependencies
```bash
pip install -r requirements.txt
```

### (Optional) Auto-install PyTorch
```bash
python -m scripts.install_torch --gpu    # or --cpu / --rocm
```

### 4) Run the server
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

> **Note:** Configuration can be set via environment variables (see "Settings").

---

## ğŸ–¥ï¸ Usage (Direct Links)
- ğŸ  **Home**: <http://localhost:8000/>
- ğŸ“š **Swagger UI**: <http://localhost:8000/docs>
- ğŸ“˜ **ReDoc**: <http://localhost:8000/redoc>
- â¤ï¸ **Health**: <http://localhost:8000/health>
- ğŸ§­ **Env Summary**: <http://localhost:8000/env>
- ğŸ”Œ **Plugins**: <http://localhost:8000/plugins>

### Quick Examples
```bash
curl http://localhost:8000/health
# {"status":"ok"}
```

Run a task in the **dummy** plugin:
```bash
curl -X POST http://localhost:8000/plugins/dummy/ping \
     -H "Content-Type: application/json" \
     -d '{"hello":"world"}'
```

Python:
```python
import requests
r = requests.post("http://localhost:8000/plugins/dummy/ping", json={"hello": "world"})
print(r.json())
```

---

## ğŸ”§ Settings (Pydantic Settings)
Environment variables prefixed with `APP_` and optionally loaded from `.env`.

| Name | Default | Description |
|---|---|---|
| `APP_APP_NAME` | `NeuroServe` | Application name |
| `APP_ENV` | `development` | `development` / `staging` / `production` |
| `APP_HOST` | `0.0.0.0` | Bind address |
| `APP_PORT` | `8000` | Port |
| `APP_RELOAD` | `true` | Auto-reload in dev mode |
| `APP_LOG_LEVEL` | `info` | Log level: `debug`/`info`/`warning`/`error` |
| `APP_DEVICE` | `cuda:0` | Device (`cuda:0`, `cpu`, `mps`, etc.) |
| `APP_MODEL_CACHE_ROOT` | `models_cache` | Cache root for models |
| `APP_HF_HOME` | auto | HuggingFace cache (within root) |
| `APP_TORCH_HOME` | auto | Torch cache |
| `APP_TRANSFORMERS_CACHE` | auto | HF hub cache |
| `APP_STATIC_DIR` | `app/static` | Static files dir |
| `APP_TEMPLATES_DIR` | `app/templates` | Templates dir |
| `APP_UPLOAD_DIR` | `uploads` | Upload dir |
| `APP_CORS_ALLOW_ORIGINS` | `[*]` | CORS origins |
| `APP_CORS_ALLOW_METHODS` | `[*]` | CORS methods |
| `APP_CORS_ALLOW_HEADERS` | `[*]` | CORS headers |
| `APP_CORS_ALLOW_CREDENTIALS` | `false` | Allow cookies |
| `APP_DB_URL` | â€” | Database URL (optional) |
| `APP_JWT_*` | â€” | JWT settings (optional) |

---

## ğŸ”Œ Plugins System
Each plugin lives under `app/plugins/<name>` and usually includes:
```
manifest.json
plugin.py        # defines a Plugin class inheriting AIPlugin
README.md        # plugin docs
```

API Endpoints:
- `GET /plugins` â€” list all loaded plugins with metadata.
- `POST /plugins/{name}/{task}` â€” run a task in the specified plugin.

Example `plugin.py`:
```python
from app.plugins.base import AIPlugin

class Plugin(AIPlugin):
    name = "my_plugin"
    tasks = ["infer"]

    def load(self) -> None:
        # Load models/resources once
        ...

    def infer(self, payload: dict) -> dict:
        return {"task": payload.get("task"), "message": "ok", "payload_received": payload}
```

---

## ğŸ§ª Development & Testing
Install dev requirements:
```bash
pip install -r requirements-dev.txt
pre-commit install
```

Run tests:
```bash
pytest
```

Ruff & formatting run automatically via pre-commit hooks.

---

## ğŸ“¦ Prefetch Models
Download supported models into `models_cache/`:
```bash
python -m scripts.prefetch_models
```
(See `docs/LICENSES.md` for model licenses.)

---

## ğŸ§° Runtime Utilities
- **CUDA info** via `app/runtime.py` (`cuda_info()`).
- **Warmup** routines for GPU readiness.

---

## ğŸ­ Deployment Notes
- Use Uvicorn/Hypercorn behind a proxy (e.g., Nginx) with multiple workers.
- Configure environment via `APP_*` vars instead of code changes.
- Adjust CORS carefully for production.

---

## ğŸ¤ Contributing
- Open **Issues** for ideas/bugs.
- Use **Pull Requests** for changes.
- Follow style (Ruff + pre-commit).

---

## ğŸ“œ License
Licensed under **MIT** â€” see [LICENSE](./LICENSE). Models may have their own licenses (see `docs/LICENSES.md`).

---

## ğŸ—ºï¸ Roadmap (Suggested)
- [ ] Add `/cuda` endpoint (return `cuda_info()`).
- [ ] Add `/warmup` endpoint.
- [ ] Provide plugin template/CLI generator.
- [ ] Support API Key/JWT authentication.
- [ ] Example plugins (translation, summarization, image classification).
