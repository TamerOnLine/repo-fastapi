# üöÄ NeuroServe ‚Äî GPU‚ÄëReady FastAPI AI Server

## üìä Project Status

| Category      | Badges |
|---------------|--------|
| **Languages** | ![Python](https://img.shields.io/badge/Python-3.12%2B-blue) ![HTML](https://img.shields.io/badge/HTML-5-orange) ![CSS](https://img.shields.io/badge/CSS-3-blueviolet) |
| **Framework** | ![FastAPI](https://img.shields.io/badge/FastAPI-0.116.x-009688) |
| **ML / GPU**  | ![PyTorch](https://img.shields.io/badge/PyTorch-2.6.x-ee4c2c) ![CUDA Ready](https://img.shields.io/badge/CUDA-Ready-76B900?logo=nvidia&logoColor=white) |
| **CI**        | [![Ubuntu CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-ubuntu.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-ubuntu.yml) [![Windows CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-windows.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-windows.yml) [![Windows GPU CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-gpu.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-gpu.yml) [![macOS CI](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-macos.yml/badge.svg)](https://github.com/TamerOnLine/repo-fastapi/actions/workflows/ci-macos.yml) |
| **License**   | ![License](https://img.shields.io/badge/License-MIT-green) |






NeuroServe is an **AI Inference Server** built on **FastAPI** and designed to run seamlessly on **GPU / CPU / ROCm / macOS MPS**.  
It provides ready-to-use REST APIs, a plugin system, prefetch utilities for models, and runtime inspection tools.

---

## ‚ú® Key Features
- üåê **Ready REST APIs** with Swagger UI (`/docs`) & ReDoc (`/redoc`).
- ‚ö° **PyTorch integration** with auto device selection (CUDA/CPU/MPS/ROCm).
- üß© **Plugin system** for loading and running models or services as standalone plugins.
- üìä **Runtime utilities**: CUDA info and warmup routines.
- üß† **Model tools**: TinyNet example model & MLP memory size calculator.
- üß± **Unified responses** with `unify_response` for consistent API outputs.

---

## üìÇ Project Structure
```text
gpu-server/
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ main.py           # FastAPI entrypoint
‚îÇ  ‚îú‚îÄ runtime.py        # Device (CUDA/CPU/MPS) management & info
‚îÇ  ‚îú‚îÄ toy_model.py      # Simple PyTorch model
‚îÇ  ‚îú‚îÄ core/             # config + logging + error handling
‚îÇ  ‚îú‚îÄ routes/           # API routes (plugins, uploads, ...)
‚îÇ  ‚îú‚îÄ plugins/          # plugin system (dummy, neu_server)
‚îÇ  ‚îú‚îÄ templates/        # index.html
‚îÇ  ‚îî‚îÄ static/           # style.css, favicon.ico
‚îú‚îÄ scripts/             # helper scripts (install_torch, prefetch_models, tests)
‚îú‚îÄ models_cache/        # HF/Torch model cache
‚îú‚îÄ docs/                # model licenses & docs
‚îú‚îÄ tests/               # optional tests
‚îú‚îÄ requirements*.txt
‚îú‚îÄ pyproject.toml       # Ruff/pytest/coverage configs
‚îî‚îÄ README.md , LICENSE
```

---

## ‚öôÔ∏è Quick Installation
### 1) Clone the repository
```bash
git clone https://github.com/USERNAME/gpu-server.git
cd gpu-server
```

### 2) Create virtual environment
```bash
python -m venv .venv
# Linux/macOS
source .venv/bin/activate
# Windows
.venv\Scripts\activate
```

### 3) Install dependencies
```bash
pip install -r requirements.txt
```

### (Optional) Auto-install PyTorch
```bash
python -m scripts.install_torch --gpu    # or --cpu / --rocm
```

### 4) Run the server
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

> **Note:** Configuration can be set via environment variables (see "Settings").

---

## üñ•Ô∏è Usage (Direct Links)
- üè† **Home**: <http://localhost:8000/>
- üìö **Swagger UI**: <http://localhost:8000/docs>
- üìò **ReDoc**: <http://localhost:8000/redoc>
- ‚ù§Ô∏è **Health**: <http://localhost:8000/health>
- üß≠ **Env Summary**: <http://localhost:8000/env>
- üîå **Plugins**: <http://localhost:8000/plugins>

### Quick Examples
```bash
curl http://localhost:8000/health
# {"status":"ok"}
```

Run a task in the **dummy** plugin:
```bash
curl -X POST http://localhost:8000/plugins/dummy/ping \
     -H "Content-Type: application/json" \
     -d '{"hello":"world"}'
```

Python:
```python
import requests
r = requests.post("http://localhost:8000/plugins/dummy/ping", json={"hello": "world"})
print(r.json())
```

---

## üîß Settings (Pydantic Settings)
Environment variables prefixed with `APP_` and optionally loaded from `.env`.

| Name | Default | Description |
|---|---|---|
| `APP_APP_NAME` | `NeuroServe` | Application name |
| `APP_ENV` | `development` | `development` / `staging` / `production` |
| `APP_HOST` | `0.0.0.0` | Bind address |
| `APP_PORT` | `8000` | Port |
| `APP_RELOAD` | `true` | Auto-reload in dev mode |
| `APP_LOG_LEVEL` | `info` | Log level: `debug`/`info`/`warning`/`error` |
| `APP_DEVICE` | `cuda:0` | Device (`cuda:0`, `cpu`, `mps`, etc.) |
| `APP_MODEL_CACHE_ROOT` | `models_cache` | Cache root for models |
| `APP_HF_HOME` | auto | HuggingFace cache (within root) |
| `APP_TORCH_HOME` | auto | Torch cache |
| `APP_TRANSFORMERS_CACHE` | auto | HF hub cache |
| `APP_STATIC_DIR` | `app/static` | Static files dir |
| `APP_TEMPLATES_DIR` | `app/templates` | Templates dir |
| `APP_UPLOAD_DIR` | `uploads` | Upload dir |
| `APP_CORS_ALLOW_ORIGINS` | `[*]` | CORS origins |
| `APP_CORS_ALLOW_METHODS` | `[*]` | CORS methods |
| `APP_CORS_ALLOW_HEADERS` | `[*]` | CORS headers |
| `APP_CORS_ALLOW_CREDENTIALS` | `false` | Allow cookies |
| `APP_DB_URL` | ‚Äî | Database URL (optional) |
| `APP_JWT_*` | ‚Äî | JWT settings (optional) |

---

## üîå Plugins System
Each plugin lives under `app/plugins/<name>` and usually includes:
```
manifest.json
plugin.py        # defines a Plugin class inheriting AIPlugin
README.md        # plugin docs
```

API Endpoints:
- `GET /plugins` ‚Äî list all loaded plugins with metadata.
- `POST /plugins/{name}/{task}` ‚Äî run a task in the specified plugin.

Example `plugin.py`:
```python
from app.plugins.base import AIPlugin

class Plugin(AIPlugin):
    name = "my_plugin"
    tasks = ["infer"]

    def load(self) -> None:
        # Load models/resources once
        ...

    def infer(self, payload: dict) -> dict:
        return {"task": payload.get("task"), "message": "ok", "payload_received": payload}
```

---

## üß™ Development & Testing
Install dev requirements:
```bash
pip install -r requirements-dev.txt
pre-commit install
```

Run tests:
```bash
pytest
```

Ruff & formatting run automatically via pre-commit hooks.

---

## üì¶ Prefetch Models
Download supported models into `models_cache/`:
```bash
python -m scripts.prefetch_models
```
(See `docs/LICENSES.md` for model licenses.)

---

## üß∞ Runtime Utilities
- **CUDA info** via `app/runtime.py` (`cuda_info()`).
- **Warmup** routines for GPU readiness.

---

## üè≠ Deployment Notes
- Use Uvicorn/Hypercorn behind a proxy (e.g., Nginx) with multiple workers.
- Configure environment via `APP_*` vars instead of code changes.
- Adjust CORS carefully for production.

---

## ü§ù Contributing
- Open **Issues** for ideas/bugs.
- Use **Pull Requests** for changes.
- Follow style (Ruff + pre-commit).

---

## üìú License
Licensed under **MIT** ‚Äî see [LICENSE](./LICENSE). Models may have their own licenses (see `docs/LICENSES.md`).

---

## üó∫Ô∏è Roadmap (Suggested)
- [ ] Add `/cuda` endpoint (return `cuda_info()`).
- [ ] Add `/warmup` endpoint.
- [ ] Provide plugin template/CLI generator.
- [ ] Support API Key/JWT authentication.
- [ ] Example plugins (translation, summarization, image classification).
